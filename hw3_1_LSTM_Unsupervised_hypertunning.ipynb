{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-tuner in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (1.4.7)\n",
      "Requirement already satisfied: keras in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from keras-tuner) (3.5.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from keras-tuner) (24.1)\n",
      "Requirement already satisfied: requests in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from keras-tuner) (2.32.3)\n",
      "Requirement already satisfied: kt-legacy in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from keras-tuner) (1.0.5)\n",
      "Requirement already satisfied: absl-py in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from keras->keras-tuner) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from keras->keras-tuner) (1.26.4)\n",
      "Requirement already satisfied: rich in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from keras->keras-tuner) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from keras->keras-tuner) (0.0.8)\n",
      "Requirement already satisfied: h5py in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from keras->keras-tuner) (3.12.1)\n",
      "Requirement already satisfied: optree in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from keras->keras-tuner) (0.12.1)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from keras->keras-tuner) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from requests->keras-tuner) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from requests->keras-tuner) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from requests->keras-tuner) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from requests->keras-tuner) (2024.7.4)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from optree->keras->keras-tuner) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from rich->keras->keras-tuner) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from rich->keras->keras-tuner) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np # to use numpy arrays instead of lists\n",
    "import pandas as pd # DataFrame (table)\n",
    "import matplotlib.pyplot as plt # to plot\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import text\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense\n",
    "from tensorflow.keras.layers import LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "!pip install keras-tuner \n",
    "import keras_tuner as kt \n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData():\n",
    "    corpus = \"\"\"\n",
    "        It was a bright cold day in April, and the clocks were striking thirteen.\\n\n",
    "        Winston Smith, his chin nuzzled into his breast in an effort to escape the vile wind, slipped quickly through the glass doors of Victory Mansions, though not quickly enough to prevent a swirl of gritty dust from entering along with him.\\n\n",
    "        The hallway smelt of boiled cabbage and old rag mats.\\n\n",
    "        At one end of it a coloured poster, too large for indoor display, had been tacked to the wall.\\n\n",
    "        It depicted simply an enormous face, more than a metre wide: the face of a man of about forty-five, with a heavy black moustache and ruggedly handsome features.\\n\n",
    "        Winston made for the stairs.\\n\n",
    "        It was no use trying the lift.\\n\n",
    "        Even at the best of times it was seldom working, and at present the electric current was cut off during daylight hours.\\n\n",
    "        It was part of the economy drive in preparation for Hate Week.\\n\n",
    "        The flat was seven flights up, and Winston, who was thirty-nine and had a varicose ulcer above his right ankle, went slowly, resting several times on the way.\\n\n",
    "        On each landing, opposite the lift-shaft, the poster with the enormous face gazed from the wall.\\n\n",
    "        It was one of those pictures which are so contrived that the eyes follow you about when you move.\\n\n",
    "        BIG BROTHER IS WATCHING YOU, the caption beneath it ran.\\n\n",
    "        Inside the flat a fruity voice was reading out a list of figures which had something to do with the production of pig-iron.\\n\n",
    "        The voice came from an oblong metal plaque like a dulled mirror which formed part of the surface of the right-hand wall.\\n\n",
    "        Winston turned a switch and the voice sank somewhat, though the words were still distinguishable.\\n\n",
    "        The instrument (the telescreen, it was called) could be dimmed, but there was no way of shutting it off completely.\\n\n",
    "        He moved over to the window: a smallish, frail figure, the meagreness of his body merely emphasized by the blue overalls which were the uniform of the party.\\n\n",
    "        His hair was very fair, his face naturally sanguine, his skin roughened by coarse soap and blunt razor blades and the cold of the winter that had just ended.\\n\n",
    "        Outside, even through the shut window-pane, the world looked cold.\\n\n",
    "        Down in the street little eddies of wind were whirling dust and torn paper into spirals, and though the sun was shining and the sky a harsh blue, there seemed to be no colour in anything, except the posters that were plastered everywhere.\\n\n",
    "        The black-moustachio’d face gazed down from every commanding corner.\\n\n",
    "        There was one on the house-front immediately opposite.\\n\n",
    "        BIG BROTHER IS WATCHING YOU, the caption said, while the dark eyes looked deep into Winston’s own.\\n\n",
    "        Down at streetlevel another poster, torn at one corner, flapped fitfully in the wind, alternately covering and uncovering the single word INGSOC.\\n\n",
    "        In the far distance a helicopter skimmed down between the roofs, hovered for an instant like a bluebottle, and darted away again with a curving flight.\\n\n",
    "        It was the police patrol, snooping into people’s windows.\\n\n",
    "        The patrols did not matter, however.\\n\n",
    "        Only the Thought Police mattered.\\n\n",
    "        Behind Winston’s back the voice from the telescreen was still babbling away about pig-iron and the overfulfilment of the Ninth Three-Year Plan.\\n\n",
    "        The telescreen received and transmitted simultaneously.\\n\n",
    "        Any sound that Winston made, above the level of a very low whisper, would be picked up by it, moreover, so long as he remained within the field of vision which the metal plaque commanded, he could be seen as well as heard.\\n\n",
    "        There was of course no way of knowing whether you were being watched at any given moment.\\n\n",
    "        How often, or on what system, the Thought Police plugged in on any individual wire was guesswork.\\n\n",
    "        It was even conceivable that they watched everybody all the time.\\n\n",
    "        But at any rate they could plug in your wire whenever they wanted to.\\n\n",
    "        You had to live — did live, from habit that became instinct — in the assumption that every sound you made was overheard, and, except in darkness, every movement scrutinized.\\n\n",
    "        Winston kept his back turned to the telescreen.\\n\n",
    "        It was safer, though, as he well knew, even a back can be revealing.\\n\n",
    "        A kilometre away the Ministry of Truth, his place of work, towered vast and white above the grimy landscape.\\n\n",
    "        This, he thought with a sort of vague distaste — this was London, chief city of Airstrip One, itself the third most populous of the provinces of Oceania.\\n\n",
    "        He tried to squeeze out some childhood memory that should tell him whether London had always been quite like this.\\n\n",
    "        Were there always these vistas of rotting nineteenth-century houses, their sides shored up with baulks of timber, their windows patched with cardboard and their roofs with corrugated iron, their crazy garden walls sagging in all directions?\\n\n",
    "        And the bombed sites where the plaster dust swirled in the air and the willow-herb straggled over the heaps of rubble; and the places where the bombs had cleared a larger patch and there had sprung up sordid colonies of wooden dwellings like chicken-houses?\\n\n",
    "        But it was no use, he could not remember: nothing remained of his childhood except a series of bright-lit tableaux occurring against no background and mostly unintelligible.\\n\n",
    "        The Ministry of Truth — Minitrue, in Newspeak* — was startlingly different from any other object in sight.\\n\n",
    "        It was an enormous pyramidal structure of glittering white concrete, soaring up, terrace after terrace, 300 metres into the air.\\n\n",
    "        From where Winston stood it was just possible to read, picked out on its white face in elegant lettering, the three slogans of the Party:\\n\n",
    "        WAR IS PEACE\\n\n",
    "        FREEDOM IS SLAVERY\\n\n",
    "        IGNORANCE IS STRENGTH\\n\n",
    "        The Ministry of Truth contained, it was said, three thousand rooms above ground level, and corresponding ramifications below.\\n\n",
    "        Scattered about London there were just three other buildings of similar appearance and size.\\n\n",
    "        So completely did they dwarf the surrounding architecture that from the roof of Victory Mansions you could see all four of them simultaneously.\\n\n",
    "        They were the homes of the four Ministries between which the entire apparatus of government was divided.\\n\n",
    "        The Ministry of Truth, which concerned itself with news, entertainment, education, and the fine arts.\\n\n",
    "        The Ministry of Peace, which concerned itself with war.\\n\n",
    "        The Ministry of Love, which maintained law and order.\\n\n",
    "        And the Ministry of Plenty, which was responsible for economic affairs.\\n\n",
    "        Their names, in Newspeak: Minitrue, Minipax, Miniluv, and Miniplenty.\\n\n",
    "        The Ministry of Love was the really frightening one.\\n\n",
    "        There were no windows in it at all.\\n\n",
    "        Winston had never been inside the Ministry of Love, nor within half a kilometre of it.\\n\n",
    "        It was a place impossible to enter except on official business, and then only by penetrating through a maze of barbed-wire entanglements, steel doors, and hidden machine-gun nests.\\n\n",
    "        Even the streets leading up to its outer barriers were roamed by gorilla-faced guards in black uniforms, armed with jointed truncheons.\\n\n",
    "        Winston turned round abruptly.\\n\n",
    "        He had set his features into the expression of quiet optimism which it was advisable to wear when facing the telescreen.\\n\n",
    "        He crossed the room into the tiny kitchen.\\n\n",
    "        By leaving the Ministry at this time of day he had sacrificed his lunch in the canteen, and he was aware that there was no food in the kitchen except a hunk of dark-coloured bread which had got to be saved for tomorrow’s breakfast.\\n\n",
    "        He took down from the shelf a bottle of colourless liquid with a plain white label marked VICTORY GIN.\\n\n",
    "        It gave off a sickly, oily smell, as of Chinese rice-spirit.\\n\n",
    "        Winston poured out nearly a teacupful, nerved himself for a shock, and gulped it down like a dose of medicine.\\n\n",
    "        Instantly his face turned scarlet and the water ran out of his eyes.\\n\n",
    "        The stuff was like nitric acid, and moreover, in swallowing it one had the sensation of being hit on the back of the head with a rubber club.\\n\n",
    "        \"\"\"\n",
    "    dataSet = corpus.split(\"\\n\")\n",
    "    return dataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'of': 2, 'and': 3, 'was': 4, 'a': 5, 'it': 6, 'in': 7, 'to': 8, 'his': 9, 'with': 10, 'had': 11, 'which': 12, 'he': 13, 'were': 14, 'winston': 15, 'from': 16, 'that': 17, 'ministry': 18, 'at': 19, 'there': 20, 'no': 21, 'on': 22, 'you': 23, 'into': 24, 'one': 25, 'for': 26, 'face': 27, 'up': 28, 'like': 29, 'be': 30, 'by': 31, 'down': 32, 'an': 33, 'even': 34, 'is': 35, 'out': 36, 'telescreen': 37, 'could': 38, 'except': 39, 'any': 40, 'as': 41, 'they': 42, '—': 43, 'their': 44, 'though': 45, 'about': 46, 'above': 47, 'voice': 48, 'turned': 49, 'back': 50, 'three': 51, 'all': 52, 'truth': 53, 'white': 54, 'this': 55, 'cold': 56, 'wind': 57, 'through': 58, 'victory': 59, 'not': 60, 'dust': 61, 'poster': 62, 'been': 63, 'wall': 64, 'enormous': 65, 'black': 66, 'made': 67, 'off': 68, 'way': 69, 'so': 70, 'eyes': 71, 'iron': 72, 'but': 73, 'just': 74, 'every': 75, 'away': 76, 'police': 77, 'windows': 78, 'did': 79, 'thought': 80, 'wire': 81, 'london': 82, 'itself': 83, 'where': 84, 'love': 85, 'bright': 86, 'day': 87, 'quickly': 88, 'doors': 89, 'mansions': 90, 'him': 91, 'coloured': 92, 'features': 93, 'use': 94, 'lift': 95, 'times': 96, 'part': 97, 'flat': 98, 'right': 99, 'opposite': 100, 'gazed': 101, 'when': 102, 'big': 103, 'brother': 104, 'watching': 105, 'caption': 106, 'ran': 107, 'inside': 108, 'pig': 109, 'metal': 110, 'plaque': 111, 'still': 112, 'completely': 113, 'over': 114, 'window': 115, 'blue': 116, 'party': 117, 'very': 118, 'looked': 119, 'torn': 120, 'corner': 121, 'said': 122, 'dark': 123, 'winston’s': 124, 'between': 125, 'roofs': 126, 'only': 127, 'simultaneously': 128, 'sound': 129, 'level': 130, 'picked': 131, 'moreover': 132, 'remained': 133, 'within': 134, 'well': 135, 'whether': 136, 'being': 137, 'watched': 138, 'time': 139, 'live': 140, 'kilometre': 141, 'place': 142, 'childhood': 143, 'always': 144, 'houses': 145, 'air': 146, 'minitrue': 147, 'newspeak': 148, 'other': 149, 'terrace': 150, 'its': 151, 'war': 152, 'peace': 153, 'four': 154, 'concerned': 155, 'kitchen': 156, 'april': 157, 'clocks': 158, 'striking': 159, 'thirteen': 160, 'smith': 161, 'chin': 162, 'nuzzled': 163, 'breast': 164, 'effort': 165, 'escape': 166, 'vile': 167, 'slipped': 168, 'glass': 169, 'enough': 170, 'prevent': 171, 'swirl': 172, 'gritty': 173, 'entering': 174, 'along': 175, 'hallway': 176, 'smelt': 177, 'boiled': 178, 'cabbage': 179, 'old': 180, 'rag': 181, 'mats': 182, 'end': 183, 'too': 184, 'large': 185, 'indoor': 186, 'display': 187, 'tacked': 188, 'depicted': 189, 'simply': 190, 'more': 191, 'than': 192, 'metre': 193, 'wide': 194, 'man': 195, 'forty': 196, 'five': 197, 'heavy': 198, 'moustache': 199, 'ruggedly': 200, 'handsome': 201, 'stairs': 202, 'trying': 203, 'best': 204, 'seldom': 205, 'working': 206, 'present': 207, 'electric': 208, 'current': 209, 'cut': 210, 'during': 211, 'daylight': 212, 'hours': 213, 'economy': 214, 'drive': 215, 'preparation': 216, 'hate': 217, 'week': 218, 'seven': 219, 'flights': 220, 'who': 221, 'thirty': 222, 'nine': 223, 'varicose': 224, 'ulcer': 225, 'ankle': 226, 'went': 227, 'slowly': 228, 'resting': 229, 'several': 230, 'each': 231, 'landing': 232, 'shaft': 233, 'those': 234, 'pictures': 235, 'are': 236, 'contrived': 237, 'follow': 238, 'move': 239, 'beneath': 240, 'fruity': 241, 'reading': 242, 'list': 243, 'figures': 244, 'something': 245, 'do': 246, 'production': 247, 'came': 248, 'oblong': 249, 'dulled': 250, 'mirror': 251, 'formed': 252, 'surface': 253, 'hand': 254, 'switch': 255, 'sank': 256, 'somewhat': 257, 'words': 258, 'distinguishable': 259, 'instrument': 260, 'called': 261, 'dimmed': 262, 'shutting': 263, 'moved': 264, 'smallish': 265, 'frail': 266, 'figure': 267, 'meagreness': 268, 'body': 269, 'merely': 270, 'emphasized': 271, 'overalls': 272, 'uniform': 273, 'hair': 274, 'fair': 275, 'naturally': 276, 'sanguine': 277, 'skin': 278, 'roughened': 279, 'coarse': 280, 'soap': 281, 'blunt': 282, 'razor': 283, 'blades': 284, 'winter': 285, 'ended': 286, 'outside': 287, 'shut': 288, 'pane': 289, 'world': 290, 'street': 291, 'little': 292, 'eddies': 293, 'whirling': 294, 'paper': 295, 'spirals': 296, 'sun': 297, 'shining': 298, 'sky': 299, 'harsh': 300, 'seemed': 301, 'colour': 302, 'anything': 303, 'posters': 304, 'plastered': 305, 'everywhere': 306, 'moustachio’d': 307, 'commanding': 308, 'house': 309, 'front': 310, 'immediately': 311, 'while': 312, 'deep': 313, 'own': 314, 'streetlevel': 315, 'another': 316, 'flapped': 317, 'fitfully': 318, 'alternately': 319, 'covering': 320, 'uncovering': 321, 'single': 322, 'word': 323, 'ingsoc': 324, 'far': 325, 'distance': 326, 'helicopter': 327, 'skimmed': 328, 'hovered': 329, 'instant': 330, 'bluebottle': 331, 'darted': 332, 'again': 333, 'curving': 334, 'flight': 335, 'patrol': 336, 'snooping': 337, 'people’s': 338, 'patrols': 339, 'matter': 340, 'however': 341, 'mattered': 342, 'behind': 343, 'babbling': 344, 'overfulfilment': 345, 'ninth': 346, 'year': 347, 'plan': 348, 'received': 349, 'transmitted': 350, 'low': 351, 'whisper': 352, 'would': 353, 'long': 354, 'field': 355, 'vision': 356, 'commanded': 357, 'seen': 358, 'heard': 359, 'course': 360, 'knowing': 361, 'given': 362, 'moment': 363, 'how': 364, 'often': 365, 'or': 366, 'what': 367, 'system': 368, 'plugged': 369, 'individual': 370, 'guesswork': 371, 'conceivable': 372, 'everybody': 373, 'rate': 374, 'plug': 375, 'your': 376, 'whenever': 377, 'wanted': 378, 'habit': 379, 'became': 380, 'instinct': 381, 'assumption': 382, 'overheard': 383, 'darkness': 384, 'movement': 385, 'scrutinized': 386, 'kept': 387, 'safer': 388, 'knew': 389, 'can': 390, 'revealing': 391, 'work': 392, 'towered': 393, 'vast': 394, 'grimy': 395, 'landscape': 396, 'sort': 397, 'vague': 398, 'distaste': 399, 'chief': 400, 'city': 401, 'airstrip': 402, 'third': 403, 'most': 404, 'populous': 405, 'provinces': 406, 'oceania': 407, 'tried': 408, 'squeeze': 409, 'some': 410, 'memory': 411, 'should': 412, 'tell': 413, 'quite': 414, 'these': 415, 'vistas': 416, 'rotting': 417, 'nineteenth': 418, 'century': 419, 'sides': 420, 'shored': 421, 'baulks': 422, 'timber': 423, 'patched': 424, 'cardboard': 425, 'corrugated': 426, 'crazy': 427, 'garden': 428, 'walls': 429, 'sagging': 430, 'directions': 431, 'bombed': 432, 'sites': 433, 'plaster': 434, 'swirled': 435, 'willow': 436, 'herb': 437, 'straggled': 438, 'heaps': 439, 'rubble': 440, 'places': 441, 'bombs': 442, 'cleared': 443, 'larger': 444, 'patch': 445, 'sprung': 446, 'sordid': 447, 'colonies': 448, 'wooden': 449, 'dwellings': 450, 'chicken': 451, 'remember': 452, 'nothing': 453, 'series': 454, 'lit': 455, 'tableaux': 456, 'occurring': 457, 'against': 458, 'background': 459, 'mostly': 460, 'unintelligible': 461, 'startlingly': 462, 'different': 463, 'object': 464, 'sight': 465, 'pyramidal': 466, 'structure': 467, 'glittering': 468, 'concrete': 469, 'soaring': 470, 'after': 471, '300': 472, 'metres': 473, 'stood': 474, 'possible': 475, 'read': 476, 'elegant': 477, 'lettering': 478, 'slogans': 479, 'freedom': 480, 'slavery': 481, 'ignorance': 482, 'strength': 483, 'contained': 484, 'thousand': 485, 'rooms': 486, 'ground': 487, 'corresponding': 488, 'ramifications': 489, 'below': 490, 'scattered': 491, 'buildings': 492, 'similar': 493, 'appearance': 494, 'size': 495, 'dwarf': 496, 'surrounding': 497, 'architecture': 498, 'roof': 499, 'see': 500, 'them': 501, 'homes': 502, 'ministries': 503, 'entire': 504, 'apparatus': 505, 'government': 506, 'divided': 507, 'news': 508, 'entertainment': 509, 'education': 510, 'fine': 511, 'arts': 512, 'maintained': 513, 'law': 514, 'order': 515, 'plenty': 516, 'responsible': 517, 'economic': 518, 'affairs': 519, 'names': 520, 'minipax': 521, 'miniluv': 522, 'miniplenty': 523, 'really': 524, 'frightening': 525, 'never': 526, 'nor': 527, 'half': 528, 'impossible': 529, 'enter': 530, 'official': 531, 'business': 532, 'then': 533, 'penetrating': 534, 'maze': 535, 'barbed': 536, 'entanglements': 537, 'steel': 538, 'hidden': 539, 'machine': 540, 'gun': 541, 'nests': 542, 'streets': 543, 'leading': 544, 'outer': 545, 'barriers': 546, 'roamed': 547, 'gorilla': 548, 'faced': 549, 'guards': 550, 'uniforms': 551, 'armed': 552, 'jointed': 553, 'truncheons': 554, 'round': 555, 'abruptly': 556, 'set': 557, 'expression': 558, 'quiet': 559, 'optimism': 560, 'advisable': 561, 'wear': 562, 'facing': 563, 'crossed': 564, 'room': 565, 'tiny': 566, 'leaving': 567, 'sacrificed': 568, 'lunch': 569, 'canteen': 570, 'aware': 571, 'food': 572, 'hunk': 573, 'bread': 574, 'got': 575, 'saved': 576, 'tomorrow’s': 577, 'breakfast': 578, 'took': 579, 'shelf': 580, 'bottle': 581, 'colourless': 582, 'liquid': 583, 'plain': 584, 'label': 585, 'marked': 586, 'gin': 587, 'gave': 588, 'sickly': 589, 'oily': 590, 'smell': 591, 'chinese': 592, 'rice': 593, 'spirit': 594, 'poured': 595, 'nearly': 596, 'teacupful': 597, 'nerved': 598, 'himself': 599, 'shock': 600, 'gulped': 601, 'dose': 602, 'medicine': 603, 'instantly': 604, 'scarlet': 605, 'water': 606, 'stuff': 607, 'nitric': 608, 'acid': 609, 'swallowing': 610, 'sensation': 611, 'hit': 612, 'head': 613, 'rubber': 614, 'club': 615}\n",
      "616\n"
     ]
    }
   ],
   "source": [
    "fragment = readData()\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(fragment)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "print(tokenizer.word_index)\n",
    "print(total_words)\n",
    "\n",
    "input_sequences = []\n",
    "for line in fragment:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding=\"pre\"))\n",
    "\n",
    "xs = input_sequences[:,:-1]\n",
    "labels = input_sequences[:,-1]\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 28 Complete [00h 00m 24s]\n",
      "val_accuracy: 0.05882352963089943\n",
      "\n",
      "Best val_accuracy So Far: 0.09803921729326248\n",
      "Total elapsed time: 00h 05m 54s\n",
      "\n",
      "The hyperparameter search is complete. The optimal number of units in the first LSTM layer is 160 and the optimal number of units in the second LSTM layer is 224.\n",
      "\n",
      "Epoch 1/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.0452 - loss: 6.3374 - val_accuracy: 0.0588 - val_loss: 5.9482\n",
      "Epoch 2/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.0788 - loss: 5.6966 - val_accuracy: 0.0588 - val_loss: 6.0887\n",
      "Epoch 3/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - accuracy: 0.0685 - loss: 5.7272 - val_accuracy: 0.0588 - val_loss: 6.2839\n",
      "Epoch 4/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - accuracy: 0.0634 - loss: 5.5736 - val_accuracy: 0.0588 - val_loss: 6.3393\n",
      "Epoch 5/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.0789 - loss: 5.5978 - val_accuracy: 0.0588 - val_loss: 6.4761\n",
      "Epoch 6/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - accuracy: 0.0793 - loss: 5.5157 - val_accuracy: 0.0627 - val_loss: 6.4852\n",
      "Epoch 7/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.0796 - loss: 5.4010 - val_accuracy: 0.0745 - val_loss: 6.5553\n",
      "Epoch 8/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.1005 - loss: 5.2121 - val_accuracy: 0.1098 - val_loss: 6.5299\n",
      "Epoch 9/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.1176 - loss: 5.1048 - val_accuracy: 0.1098 - val_loss: 6.6350\n",
      "Epoch 10/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.1355 - loss: 4.9813 - val_accuracy: 0.1020 - val_loss: 6.7201\n",
      "Epoch 11/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.1419 - loss: 4.8308 - val_accuracy: 0.1137 - val_loss: 6.7652\n",
      "Epoch 12/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.1483 - loss: 4.6237 - val_accuracy: 0.1020 - val_loss: 6.8700\n",
      "Epoch 13/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.1492 - loss: 4.6104 - val_accuracy: 0.1137 - val_loss: 6.9608\n",
      "Epoch 14/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.1744 - loss: 4.2841 - val_accuracy: 0.1059 - val_loss: 7.0252\n",
      "Epoch 15/50\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.1689 - loss: 4.2322 - val_accuracy: 0.1098 - val_loss: 7.0731\n",
      "Epoch 16/50\n",
      "\u001b[1m 5/32\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - accuracy: 0.1997 - loss: 3.8535"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 30\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124mThe hyperparameter search is complete. The optimal number of units in the first LSTM layer is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_hps\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munits_0\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and the optimal number of units in the second LSTM layer is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_hps\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munits_1\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m model \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mhypermodel\u001b[38;5;241m.\u001b[39mbuild(best_hps)\n\u001b[1;32m---> 30\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\efren\\Documents\\GitHub\\venvNaturalLenguageProcessing\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\efren\\Documents\\GitHub\\venvNaturalLenguageProcessing\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:322\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    320\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    321\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m--> 322\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\efren\\Documents\\GitHub\\venvNaturalLenguageProcessing\\Lib\\site-packages\\keras\\src\\callbacks\\callback_list.py:103\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    101\u001b[0m         callback\u001b[38;5;241m.\u001b[39mon_train_batch_begin(batch, logs\u001b[38;5;241m=\u001b[39mlogs)\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    104\u001b[0m     logs \u001b[38;5;241m=\u001b[39m logs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, hp.Int('embedding_dim', min_value=50, max_value=600, step=50), input_length=max_sequence_len-1))\n",
    "    for i in range(hp.Int('num_layers', 1, 3)):\n",
    "        model.add(LSTM(units=hp.Int('units_' + str(i), min_value=32, max_value=256, step=32), return_sequences=True if i < hp.Int('num_layers', 1, 3) - 1 else False))\n",
    "        model.add(Dropout(hp.Float('dropout_' + str(i), min_value=0.2, max_value=0.5, step=0.1)))\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "tuner = kt.RandomSearch(build_model,\n",
    "                        objective='val_accuracy',\n",
    "                        max_trials=10,  # Limit the number of trials\n",
    "                        executions_per_trial=1,\n",
    "                        directory='my_dir',\n",
    "                        project_name='intro_to_kt')\n",
    "\n",
    "stop_early = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "tuner.search(xs, ys, epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first LSTM layer is {best_hps.get('units_0')} and the optimal number of units in the second LSTM layer is {best_hps.get('units_1')}.\n",
    "\"\"\")\n",
    "\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(xs, ys, epochs=50, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.add_subplot(121)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title(\"Loss vs Epochs\")\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "fig.add_subplot(122)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title(\"Accuracy vs Epochs\")\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = \"Whether 'tis nobler in the mind to suffer \\n The slings and arrows of outrageous fortune\"\n",
    "next_words = 40\n",
    "\n",
    "for i in range(next_words):\n",
    "  token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "  token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding=\"pre\")\n",
    "  predicted = model.predict(token_list, verbose=0)\n",
    "  output_word = \"\"\n",
    "  predicted = np.argmax(predicted, axis=-1)\n",
    "  for word, index in tokenizer.word_index.items():\n",
    "      if index in predicted:\n",
    "          output_word = word\n",
    "          break\n",
    "  seed_text += \" \" + output_word\n",
    "\n",
    "print(seed_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvNaturalLenguageProcessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
