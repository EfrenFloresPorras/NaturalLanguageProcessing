{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\efren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\efren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\efren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es-core-news-sm==3.7.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from es-core-news-sm==3.7.0) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (72.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2024.7.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n",
      "Collecting en-core-web-sm==3.7.1\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (72.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.7.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import re # import re -- a module that provides support for regular expressions\n",
    "import nltk\n",
    "import nltk.data\n",
    "import spacy\n",
    "from nltk import SnowballStemmer\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "!python -m spacy download es_core_news_sm\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\ '\n",
      "C:\\Users\\efren\\AppData\\Local\\Temp\\ipykernel_18388\\2866532626.py:5: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  text3 = \"Ethics are built right into the ideals and objectives of the United Nations \\ #UN #UNSG google.com bit.ly/2guVelr\"\n"
     ]
    }
   ],
   "source": [
    "# text in English\n",
    "# text\n",
    "text1 = \"Hello, Everybody. #Welcome to the NLP class, hope you enjoy it a lot. @ Remember to do all your activities and assigned homeworks. Have a nice day!!\"\n",
    "text2 = \"We don't read and write poetry because it's cute. We read and write poetry because we are members of the human race. And the human race is filled with passion. And medicine, law, business, engineering, these are noble pursuits and necessary to sustain life. But poetry, beauty, romance, love, these are what we stay alive for. To quote from Whitman, O me! O life!... of the questions of these recurring; of the endless trains of the faithless... of cities filled with the foolish; what good amid these, O me, O life? Answer. That you are here - that life exists, and identity; that the powerful play goes on and you may contribute a verse. That the powerful play goes on and you may contribute a verse. What will your verse be?\"\n",
    "text3 = \"Ethics are built right into the ideals and objectives of the United Nations \\ #UN #UNSG google.com bit.ly/2guVelr\"\n",
    "text4 = \"To be or not to be, that is the question.\"\n",
    "\n",
    "# texto\n",
    "texto1 = \"Hola a todos. Bienvenidos a la clase de LNP, espero lo disfruten mucho. Recuerden realizar todas sus actividades y tareas asignadas. Espero tengan un buen día!!\"\n",
    "texto2 = \"No escribimos poesía porque sea tierno. Leemos y escribimos poesia porque somos mienmbtros de la raza humana. Y la raza humana está llena de pasión. Y la medicina, la ley, los negocios, la ingeniería, son actividades nobles y necesarias para sostener la vida. Pero la poesía, la belleza, el romance, el amor, son por lo que vivimos. Para citar a Whitman, ¡Oh yo! ¡Oh vida!... de las preguntas de estas recurrentes; de los interminables trenes de los infieles... de ciudades llenas de tontos; ¿qué bien hay entre estos, Oh yo, Oh vida? Respuesta. Que estás aquí - que la vida existe, y la identidad; que el poderoso juego continúa y puedes contribuir con un verso. Que el poderoso juego continúa y puedes contribuir con un verso. ¿Cuál será tu verso?\"\n",
    "texto3 = \"La ética está integrada en los ideales y objetivos de las Naciones Unidas\"\n",
    "texto4 = \"Ser o no ser, esa es la cuestión.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text1:  148\n",
      "text2:  727\n",
      "text3:  113\n",
      "text4:  41\n",
      "texto1:  160\n",
      "texto2:  749\n",
      "texto3:  73\n",
      "texto4:  33\n"
     ]
    }
   ],
   "source": [
    "print(\"text1: \", len(text1))\n",
    "print(\"text2: \", len(text2))\n",
    "print(\"text3: \", len(text3))\n",
    "print(\"text4: \", len(text4))\n",
    "\n",
    "print(\"texto1: \", len(texto1))\n",
    "print(\"texto2: \", len(texto2))\n",
    "print(\"texto3: \", len(texto3))\n",
    "print(\"texto4: \", len(texto4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return a list of the words, separating by ''\n",
    "text_split1 = text1.split(' ')\n",
    "len(text_split1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_split2 = text2.split(' ')\n",
    "len(text_split2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_split3 = text3.split(' ')\n",
    "len(text_split3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_split4 = text4.split(' ')\n",
    "len(text_split4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto_split1 = texto1.split(' ')\n",
    "len(texto_split1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto_split2 = texto2.split(' ')\n",
    "len(texto_split2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto_split3 = texto3.split(' ')\n",
    "len(texto_split3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto_split4 = texto4.split(' ')\n",
    "len(texto_split4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello,', 'Everybody.', '#Welcome', 'Remember', 'Have']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List comprehension allows us to find specific words:\n",
    "# Capitalized words\n",
    "[word for word in text_split1 if word.istitle()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"it's\",\n",
       " 'members',\n",
       " 'is',\n",
       " 'pursuits',\n",
       " 'questions',\n",
       " 'endless',\n",
       " 'trains',\n",
       " 'cities',\n",
       " 'goes',\n",
       " 'goes']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Words that end in 's'\n",
    "[word for word in text_split2 if word.endswith('s')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_split3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'To', 'be', 'be,', 'is', 'not', 'or', 'question.', 'that', 'the', 'to'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set of words\n",
    "set(text_split4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([word.lower() for word in text_split4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#Welcome']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding hastags\n",
    "[word for word in text_split1 if re.search(\"#\", word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding mentions\n",
    "[word for word in text_split1 if word.startswith(\"@\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['poetry',\n",
       " 'because',\n",
       " 'poetry',\n",
       " 'because',\n",
       " 'members',\n",
       " 'filled',\n",
       " 'passion.',\n",
       " 'medicine,',\n",
       " 'business,',\n",
       " 'engineering,',\n",
       " 'pursuits',\n",
       " 'necessary',\n",
       " 'sustain',\n",
       " 'poetry,',\n",
       " 'beauty,',\n",
       " 'romance,',\n",
       " 'Whitman,',\n",
       " 'life!...',\n",
       " 'questions',\n",
       " 'recurring;',\n",
       " 'endless',\n",
       " 'trains',\n",
       " 'faithless...',\n",
       " 'cities',\n",
       " 'filled',\n",
       " 'foolish;',\n",
       " 'these,',\n",
       " 'Answer.',\n",
       " 'exists,',\n",
       " 'identity;',\n",
       " 'powerful',\n",
       " 'contribute',\n",
       " 'verse.',\n",
       " 'powerful',\n",
       " 'contribute',\n",
       " 'verse.']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Words that are greater than 5 leatter long\n",
    "[word for word in text_split2 if len(word) > 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we\n",
      "do\n",
      "n't\n",
      "read\n",
      "and\n",
      "write\n",
      "poetri\n",
      "becaus\n",
      "it\n",
      "'s\n",
      "cute\n",
      ".\n",
      "we\n",
      "read\n",
      "and\n",
      "write\n",
      "poetri\n",
      "becaus\n",
      "we\n",
      "are\n",
      "member\n",
      "of\n",
      "the\n",
      "human\n",
      "race\n",
      ".\n",
      "and\n",
      "the\n",
      "human\n",
      "race\n",
      "is\n",
      "fill\n",
      "with\n",
      "passion\n",
      ".\n",
      "and\n",
      "medicin\n",
      ",\n",
      "law\n",
      ",\n",
      "busi\n",
      ",\n",
      "engin\n",
      ",\n",
      "these\n",
      "are\n",
      "nobl\n",
      "pursuit\n",
      "and\n",
      "necessari\n",
      "to\n",
      "sustain\n",
      "life\n",
      ".\n",
      "but\n",
      "poetri\n",
      ",\n",
      "beauti\n",
      ",\n",
      "romanc\n",
      ",\n",
      "love\n",
      ",\n",
      "these\n",
      "are\n",
      "what\n",
      "we\n",
      "stay\n",
      "aliv\n",
      "for\n",
      ".\n",
      "to\n",
      "quot\n",
      "from\n",
      "whitman\n",
      ",\n",
      "o\n",
      "me\n",
      "!\n",
      "o\n",
      "life\n",
      "!\n",
      "...\n",
      "of\n",
      "the\n",
      "question\n",
      "of\n",
      "these\n",
      "recur\n",
      ";\n",
      "of\n",
      "the\n",
      "endless\n",
      "train\n",
      "of\n",
      "the\n",
      "faithless\n",
      "...\n",
      "of\n",
      "citi\n",
      "fill\n",
      "with\n",
      "the\n",
      "foolish\n",
      ";\n",
      "what\n",
      "good\n",
      "amid\n",
      "these\n",
      ",\n",
      "o\n",
      "me\n",
      ",\n",
      "o\n",
      "life\n",
      "?\n",
      "answer\n",
      ".\n",
      "that\n",
      "you\n",
      "are\n",
      "here\n",
      "-\n",
      "that\n",
      "life\n",
      "exist\n",
      ",\n",
      "and\n",
      "ident\n",
      ";\n",
      "that\n",
      "the\n",
      "power\n",
      "play\n",
      "goe\n",
      "on\n",
      "and\n",
      "you\n",
      "may\n",
      "contribut\n",
      "a\n",
      "vers\n",
      ".\n",
      "that\n",
      "the\n",
      "power\n",
      "play\n",
      "goe\n",
      "on\n",
      "and\n",
      "you\n",
      "may\n",
      "contribut\n",
      "a\n",
      "vers\n",
      ".\n",
      "what\n",
      "will\n",
      "your\n",
      "vers\n",
      "be\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "words = nltk.word_tokenize(text2)\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "for w in words:\n",
    "    print(stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "escrib\n",
      "poes\n",
      "porqu\n",
      "sea\n",
      "tiern\n",
      ".\n",
      "leem\n",
      "y\n",
      "escrib\n",
      "poesi\n",
      "porqu\n",
      "som\n",
      "mienmbtr\n",
      "de\n",
      "la\n",
      "raz\n",
      "human\n",
      ".\n",
      "y\n",
      "la\n",
      "raz\n",
      "human\n",
      "esta\n",
      "llen\n",
      "de\n",
      "pasion\n",
      ".\n",
      "y\n",
      "la\n",
      "medicin\n",
      ",\n",
      "la\n",
      "ley\n",
      ",\n",
      "los\n",
      "negoci\n",
      ",\n",
      "la\n",
      "ingeni\n",
      ",\n",
      "son\n",
      "activ\n",
      "nobl\n",
      "y\n",
      "necesari\n",
      "par\n",
      "sosten\n",
      "la\n",
      "vid\n",
      ".\n",
      "per\n",
      "la\n",
      "poes\n",
      ",\n",
      "la\n",
      "bellez\n",
      ",\n",
      "el\n",
      "romanc\n",
      ",\n",
      "el\n",
      "amor\n",
      ",\n",
      "son\n",
      "por\n",
      "lo\n",
      "que\n",
      "viv\n",
      ".\n",
      "par\n",
      "cit\n",
      "a\n",
      "whitm\n",
      ",\n",
      "¡oh\n",
      "yo\n",
      "!\n",
      "¡oh\n",
      "vid\n",
      "!\n",
      "...\n",
      "de\n",
      "las\n",
      "pregunt\n",
      "de\n",
      "estas\n",
      "recurrent\n",
      ";\n",
      "de\n",
      "los\n",
      "intermin\n",
      "tren\n",
      "de\n",
      "los\n",
      "infiel\n",
      "...\n",
      "de\n",
      "ciudad\n",
      "llen\n",
      "de\n",
      "tont\n",
      ";\n",
      "¿qu\n",
      "bien\n",
      "hay\n",
      "entre\n",
      "estos\n",
      ",\n",
      "oh\n",
      "yo\n",
      ",\n",
      "oh\n",
      "vid\n",
      "?\n",
      "respuest\n",
      ".\n",
      "que\n",
      "estas\n",
      "aqu\n",
      "-\n",
      "que\n",
      "la\n",
      "vid\n",
      "exist\n",
      ",\n",
      "y\n",
      "la\n",
      "ident\n",
      ";\n",
      "que\n",
      "el\n",
      "poder\n",
      "jueg\n",
      "continu\n",
      "y\n",
      "pued\n",
      "contribu\n",
      "con\n",
      "un\n",
      "vers\n",
      ".\n",
      "que\n",
      "el\n",
      "poder\n",
      "jueg\n",
      "continu\n",
      "y\n",
      "pued\n",
      "contribu\n",
      "con\n",
      "un\n",
      "vers\n",
      ".\n",
      "¿cual\n",
      "ser\n",
      "tu\n",
      "vers\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "tokens_palabras = nltk.word_tokenize(texto2)\n",
    "\n",
    "stemmer_esp = SnowballStemmer('spanish')\n",
    "for palabra in tokens_palabras:\n",
    "    print(stemmer_esp.stem(palabra))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we\n",
      "do\n",
      "not\n",
      "read\n",
      "and\n",
      "write\n",
      "poetry\n",
      "because\n",
      "it\n",
      "be\n",
      "cute\n",
      ".\n",
      "we\n",
      "read\n",
      "and\n",
      "write\n",
      "poetry\n",
      "because\n",
      "we\n",
      "be\n",
      "member\n",
      "of\n",
      "the\n",
      "human\n",
      "race\n",
      ".\n",
      "and\n",
      "the\n",
      "human\n",
      "race\n",
      "be\n",
      "fill\n",
      "with\n",
      "passion\n",
      ".\n",
      "and\n",
      "medicine\n",
      ",\n",
      "law\n",
      ",\n",
      "business\n",
      ",\n",
      "engineering\n",
      ",\n",
      "these\n",
      "be\n",
      "noble\n",
      "pursuit\n",
      "and\n",
      "necessary\n",
      "to\n",
      "sustain\n",
      "life\n",
      ".\n",
      "but\n",
      "poetry\n",
      ",\n",
      "beauty\n",
      ",\n",
      "romance\n",
      ",\n",
      "love\n",
      ",\n",
      "these\n",
      "be\n",
      "what\n",
      "we\n",
      "stay\n",
      "alive\n",
      "for\n",
      ".\n",
      "to\n",
      "quote\n",
      "from\n",
      "Whitman\n",
      ",\n",
      "o\n",
      "I\n",
      "!\n",
      "o\n",
      "life\n",
      "!\n",
      "...\n",
      "of\n",
      "the\n",
      "question\n",
      "of\n",
      "these\n",
      "recur\n",
      ";\n",
      "of\n",
      "the\n",
      "endless\n",
      "train\n",
      "of\n",
      "the\n",
      "faithless\n",
      "...\n",
      "of\n",
      "city\n",
      "fill\n",
      "with\n",
      "the\n",
      "foolish\n",
      ";\n",
      "what\n",
      "good\n",
      "amid\n",
      "these\n",
      ",\n",
      "o\n",
      "I\n",
      ",\n",
      "o\n",
      "life\n",
      "?\n",
      "answer\n",
      ".\n",
      "that\n",
      "you\n",
      "be\n",
      "here\n",
      "-\n",
      "that\n",
      "life\n",
      "exist\n",
      ",\n",
      "and\n",
      "identity\n",
      ";\n",
      "that\n",
      "the\n",
      "powerful\n",
      "play\n",
      "go\n",
      "on\n",
      "and\n",
      "you\n",
      "may\n",
      "contribute\n",
      "a\n",
      "verse\n",
      ".\n",
      "that\n",
      "the\n",
      "powerful\n",
      "play\n",
      "go\n",
      "on\n",
      "and\n",
      "you\n",
      "may\n",
      "contribute\n",
      "a\n",
      "verse\n",
      ".\n",
      "what\n",
      "will\n",
      "your\n",
      "verse\n",
      "be\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "nlp_english = spacy.load(\"en_core_web_sm\")\n",
    "for w in words:\n",
    "    doc = nlp_english(w)\n",
    "    for token in doc:\n",
    "        print(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n",
      "escribimos\n",
      "poesía\n",
      "porque\n",
      "ser\n",
      "tierno\n",
      ".\n",
      "leer\n",
      "y\n",
      "escribimos\n",
      "poesiar\n",
      "porque\n",
      "ser\n",
      "mienmbtro\n",
      "de\n",
      "el\n",
      "raza\n",
      "humano\n",
      ".\n",
      "y\n",
      "el\n",
      "raza\n",
      "humano\n",
      "estar\n",
      "lleno\n",
      "de\n",
      "pasión\n",
      ".\n",
      "y\n",
      "el\n",
      "medicina\n",
      ",\n",
      "el\n",
      "ley\n",
      ",\n",
      "el\n",
      "negocio\n",
      ",\n",
      "el\n",
      "ingeniería\n",
      ",\n",
      "ser\n",
      "actividad\n",
      "noble\n",
      "y\n",
      "necesario\n",
      "para\n",
      "sostener\n",
      "el\n",
      "vida\n",
      ".\n",
      "pero\n",
      "el\n",
      "poesía\n",
      ",\n",
      "el\n",
      "belleza\n",
      ",\n",
      "el\n",
      "romance\n",
      ",\n",
      "el\n",
      "amor\n",
      ",\n",
      "ser\n",
      "por\n",
      "él\n",
      "que\n",
      "vivir\n",
      ".\n",
      "para\n",
      "citar\n",
      "a\n",
      "Whitman\n",
      ",\n",
      "¡\n",
      "Oh\n",
      "yo\n",
      "!\n",
      "¡\n",
      "Oh\n",
      "vida\n",
      "!\n",
      "...\n",
      "de\n",
      "el\n",
      "pregunta\n",
      "de\n",
      "este\n",
      "recurrente\n",
      ";\n",
      "de\n",
      "el\n",
      "interminable\n",
      "tren\n",
      "de\n",
      "el\n",
      "infiel\n",
      "...\n",
      "de\n",
      "ciudad\n",
      "llena\n",
      "de\n",
      "tonto\n",
      ";\n",
      "¿\n",
      "qué\n",
      "bien\n",
      "haber\n",
      "entre\n",
      "este\n",
      ",\n",
      "oh\n",
      "yo\n",
      ",\n",
      "oh\n",
      "vida\n",
      "?\n",
      "Respuesta\n",
      ".\n",
      "que\n",
      "estar\n",
      "aquí\n",
      "-\n",
      "que\n",
      "el\n",
      "vida\n",
      "existir\n",
      ",\n",
      "y\n",
      "el\n",
      "identidad\n",
      ";\n",
      "que\n",
      "el\n",
      "poderoso\n",
      "juego\n",
      "continuar\n",
      "y\n",
      "poder\n",
      "contribuir\n",
      "con\n",
      "uno\n",
      "verso\n",
      ".\n",
      "que\n",
      "el\n",
      "poderoso\n",
      "juego\n",
      "continuar\n",
      "y\n",
      "poder\n",
      "contribuir\n",
      "con\n",
      "uno\n",
      "verso\n",
      ".\n",
      "¿\n",
      "cuál\n",
      "ser\n",
      "tu\n",
      "verso\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "nlp_esp = spacy.load(\"es_core_news_sm\")\n",
    "for palabra in tokens_palabras:\n",
    "    doc = nlp_esp(palabra)\n",
    "    for token in doc:\n",
    "        print(token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvNaturalLenguageProcessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
