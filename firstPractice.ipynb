{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\efren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\efren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\efren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
      "     ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 0.8/12.9 MB 6.7 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 2.6/12.9 MB 8.9 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 4.7/12.9 MB 8.9 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.9/12.9 MB 10.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 11.8/12.9 MB 12.5 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.9/12.9 MB 12.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from es-core-news-sm==3.7.0) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (72.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2024.7.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "# https://www.nltk.org/install.html\n",
    "# https://spacy.io/usage/models\n",
    "\n",
    "import nltk\n",
    "import nltk.data\n",
    "import spacy\n",
    "from nltk import SnowballStemmer\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.es.Spanish at 0x23fa2ecdf40>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.load('es_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text\n",
    "text = \"Hello, Everybody. Welcome to the NLP class, hope you enjoy it a lot. Remember to do all your activities and assigned homeworks. Have a nice day!!\"\n",
    "\n",
    "# texto\n",
    "texto = \"Hola a todos. Bienvenidos a la clase de LNP, espero lo disfruten mucho. Recuerden realizar todas sus actividades y tareas asignadas. Espero tengan un buen día!!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Everybody.\n",
      "Welcome to the NLP class, hope you enjoy it a lot.\n",
      "Remember to do all your activities and assigned homeworks.\n",
      "Have a nice day!\n",
      "!\n",
      "Hola a todos.\n",
      "Bienvenidos a la clase de LNP, espero lo disfruten mucho.\n",
      "Recuerden realizar todas sus actividades y tareas asignadas.\n",
      "Espero tengan un buen día!\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "# tokenize using sentences\n",
    "sentence = nltk.sent_tokenize(text)\n",
    "for sen in sentence:\n",
    "  print(sen)\n",
    "\n",
    "parrafo = nltk.sent_tokenize(texto)\n",
    "for parr in parrafo:\n",
    "  print(parr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      ",\n",
      "Everybody\n",
      ".\n",
      "Welcome\n",
      "to\n",
      "the\n",
      "NLP\n",
      "class\n",
      ",\n",
      "hope\n",
      "you\n",
      "enjoy\n",
      "it\n",
      "a\n",
      "lot\n",
      ".\n",
      "Remember\n",
      "to\n",
      "do\n",
      "all\n",
      "your\n",
      "activities\n",
      "and\n",
      "assigned\n",
      "homeworks\n",
      ".\n",
      "Have\n",
      "a\n",
      "nice\n",
      "day\n",
      "!\n",
      "!\n",
      "Hola\n",
      "a\n",
      "todos\n",
      ".\n",
      "Bienvenidos\n",
      "a\n",
      "la\n",
      "clase\n",
      "de\n",
      "LNP\n",
      ",\n",
      "espero\n",
      "lo\n",
      "disfruten\n",
      "mucho\n",
      ".\n",
      "Recuerden\n",
      "realizar\n",
      "todas\n",
      "sus\n",
      "actividades\n",
      "y\n",
      "tareas\n",
      "asignadas\n",
      ".\n",
      "Espero\n",
      "tengan\n",
      "un\n",
      "buen\n",
      "día\n",
      "!\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "# tokenize using words\n",
    "word = nltk.word_tokenize(text)\n",
    "for w in word:\n",
    "  print(w)\n",
    "\n",
    "palabra = nltk.word_tokenize(texto)\n",
    "for p in palabra:\n",
    "  print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Everybody\n",
      "Welcome\n",
      "to\n",
      "the\n",
      "NLP\n",
      "class\n",
      "hope\n",
      "you\n",
      "enjoy\n",
      "it\n",
      "a\n",
      "lot\n",
      "Remember\n",
      "to\n",
      "do\n",
      "all\n",
      "your\n",
      "activities\n",
      "and\n",
      "assigned\n",
      "homeworks\n",
      "Have\n",
      "a\n",
      "nice\n",
      "day\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\efren\\AppData\\Local\\Temp\\ipykernel_40776\\1563630428.py:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  word_regexp = nltk.regexp_tokenize(text, \"[\\w]+\") # \"[\\w]+\" read only words\n"
     ]
    }
   ],
   "source": [
    "# tokenize using regulat expressions\n",
    "word_regexp = nltk.regexp_tokenize(text, \"[\\w]+\") # \"[\\w]+\" read only words\n",
    "for w in word_regexp:\n",
    "  print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "global 'copy_reg._reconstructor' is forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## regular expressions for spanish, a good practice to keep in mind\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m token_esp \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtokenizers/punkt/spanish.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m enun \u001b[38;5;241m=\u001b[39m token_esp\u001b[38;5;241m.\u001b[39mtokenize(texto)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sen \u001b[38;5;129;01min\u001b[39;00m enun:\n",
      "File \u001b[1;32mc:\\Users\\efren\\Documents\\GitHub\\venvNaturalLenguageProcessing\\Lib\\site-packages\\nltk\\data.py:763\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    761\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpickle\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 763\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m \u001b[43mrestricted_pickle_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_resource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\efren\\Documents\\GitHub\\venvNaturalLenguageProcessing\\Lib\\site-packages\\nltk\\data.py:667\u001b[0m, in \u001b[0;36mrestricted_pickle_load\u001b[1;34m(string)\u001b[0m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    663\u001b[0m \u001b[38;5;124;03mPrevents any class or function from loading.\u001b[39;00m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwordnet_app\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RestrictedUnpickler\n\u001b[1;32m--> 667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRestrictedUnpickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBytesIO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\efren\\Documents\\GitHub\\venvNaturalLenguageProcessing\\Lib\\site-packages\\nltk\\app\\wordnet_app.py:664\u001b[0m, in \u001b[0;36mRestrictedUnpickler.find_class\u001b[1;34m(self, module, name)\u001b[0m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_class\u001b[39m(\u001b[38;5;28mself\u001b[39m, module, name):\n\u001b[0;32m    663\u001b[0m     \u001b[38;5;66;03m# Forbid every function\u001b[39;00m\n\u001b[1;32m--> 664\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglobal \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is forbidden\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: global 'copy_reg._reconstructor' is forbidden"
     ]
    }
   ],
   "source": [
    "## regular expressions for spanish, a good practice to keep in mind\n",
    "token_esp = nltk.data.load(\"tokenizers/punkt/spanish.pickle\")\n",
    "enun = token_esp.tokenize(texto)\n",
    "for sen in enun:\n",
    "  print(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola\n",
      "a\n",
      "todos\n",
      ".\n",
      "Bienvenidos\n",
      "a\n",
      "la\n",
      "clase\n",
      "de\n",
      "LNP\n",
      ",\n",
      "espero\n",
      "lo\n",
      "disfruten\n",
      "mucho\n",
      ".\n",
      "Recuerden\n",
      "realizar\n",
      "todas\n",
      "sus\n",
      "actividades\n",
      "y\n",
      "tareas\n",
      "asignadas\n",
      ".\n",
      "Espero\n",
      "tengan\n",
      "un\n",
      "buen\n",
      "día\n",
      "!\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "tokens_palabras = nltk.word_tokenize(texto)\n",
    "for w in tokens_palabras:\n",
    "  print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'of', 'now', 'they', \"mightn't\", 'again', 'wouldn', 'when', \"aren't\", 'other', 'but', 'herself', 's', \"needn't\", 'doesn', 'during', \"hadn't\", \"she's\", 'in', 'having', 'who', 'once', 'through', 'don', 'had', \"you're\", 'm', 'each', 'as', 'against', 'off', 'so', 'your', \"should've\", 'am', 'mustn', 'any', 'while', 'here', 'is', 'some', 'ain', 'down', 'on', 'into', 'weren', \"mustn't\", 'i', 'shouldn', 'then', \"weren't\", 'yourselves', 'isn', 'have', 'by', 'more', \"wouldn't\", 'at', 'such', \"that'll\", 'be', 'about', 'both', \"don't\", \"didn't\", 'there', 'itself', 're', 'o', 'needn', 'ma', 'what', 'not', 'and', 'hasn', 'themselves', 'until', \"you'd\", \"shan't\", 'my', \"won't\", 'to', 'd', 'the', 'you', 'are', 've', 'because', 'll', 'all', 'just', 'did', 'his', 'very', 'if', 'only', 'yours', 'it', 'own', 'her', \"wasn't\", 'further', 'haven', 'couldn', 'we', 'himself', 'its', 'from', \"shouldn't\", 'under', 'our', \"isn't\", 'most', \"you've\", 'will', 'too', 'below', 'for', 'has', 'being', 'above', 'him', 'this', 'with', 'wasn', 'hers', 'those', 'up', 'than', 'doing', 'should', 'she', 'ourselves', 'between', 'won', 'over', 'yourself', \"doesn't\", 'same', 't', 'why', 'mightn', 'their', 'were', 'didn', 'no', 'after', \"you'll\", 'myself', 'them', 'can', 'ours', 'whom', 'an', 'where', 'or', 'how', 'was', 'before', 'out', 'y', \"haven't\", 'which', 'aren', \"it's\", 'a', 'been', 'shan', 'hadn', 'theirs', 'does', 'do', \"hasn't\", 'these', 'few', \"couldn't\", 'that', 'nor', 'me', 'he'}\n"
     ]
    }
   ],
   "source": [
    "sw_english = set(nltk.corpus.stopwords.words('english'))\n",
    "print(sw_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'estoy', 'estuvieras', 'tuvierais', 'fueras', 'ni', 'estad', 'habiendo', 'tenido', 'algo', 'habría', 'habrían', 'seréis', 'nada', 'estuviste', 'habréis', 'estaré', 'seríamos', 'sin', 'tus', 'como', 'está', 'sería', 'sea', 'pero', 'suyos', 'hayan', 'e', 'o', 'eres', 'hasta', 'eran', 'estuviera', 'estuviesen', 'estuviéramos', 'hayáis', 'haya', 'mía', 'las', 'serías', 'ha', 'tenéis', 'tuvisteis', 'tendréis', 'tengo', 'estaréis', 'hubiésemos', 'has', 'tuviese', 'es', 'mí', 'del', 'os', 'serían', 'tu', 'fuerais', 'sentid', 'tened', 'hubo', 'muchos', 'sois', 'contra', 'tenga', 'tendríamos', 'mucho', 'siente', 'por', 'ante', 'habido', 'habré', 'tuve', 'todos', 'fuese', 'habremos', 'habrán', 'hubieses', 'estabais', 'estados', 'desde', 'seremos', 'eso', 'somos', 'seríais', 'con', 'nosotros', 'estaban', 'hubimos', 'serás', 'estuvieron', 'tienes', 'tendremos', 'tengamos', 'tenías', 'más', 'sentidos', 'hubierais', 'tienen', 'estaríamos', 'será', 'qué', 'tendré', 'tuviéramos', 'durante', 'algunos', 'mis', 'suya', 'estén', 'mío', 'habíamos', 'nosotras', 'esto', 'tengan', 'tuya', 'entre', 'había', 'estarás', 'esa', 'que', 'tuvieron', 'hubisteis', 'fueron', 'sus', 'tuvieseis', 'tendríais', 'hubieras', 'estábamos', 'sean', 'otros', 'estos', 'fuera', 'estás', 'estuvimos', 'serán', 'otras', 'tendría', 'fueran', 'hemos', 'habríais', 'le', 'otro', 'tuvimos', 'vosotras', 'un', 'teníais', 'habidas', 'de', 'otra', 'nuestra', 'fuisteis', 'tengas', 'esas', 'vuestras', 'hubieran', 'sí', 'fuéramos', 'tuyo', 'fui', 'tuviste', 'él', 'estáis', 'esos', 'la', 'no', 'estada', 'tiene', 'estaría', 'unos', 'estéis', 'tendrán', 'estuviese', 'estaríais', 'y', 'tuvieran', 'tenemos', 'habíais', 'todo', 'suyas', 'hubieron', 'poco', 'he', 'muy', 'estado', 'tenida', 'donde', 'suyo', 'habríamos', 'teniendo', 'hubiéramos', 'esta', 'hubieseis', 'teníamos', 'estuvieses', 'su', 'estuvierais', 'estar', 'estabas', 'están', 'quien', 'nuestro', 'se', 'hubiste', 'sentidas', 'mías', 'tuviésemos', 'estamos', 'estando', 'tuviesen', 'en', 'cual', 'estuvieran', 'nuestros', 'ella', 'vuestra', 'estuvo', 'habéis', 'tuviera', 'tuyas', 'nuestras', 'ellas', 'uno', 'hayas', 'son', 'fuimos', 'han', 'yo', 'también', 'estuve', 'tuyos', 'tendrá', 'sobre', 'lo', 'cuando', 'ellos', 'seas', 'tengáis', 'éramos', 'ti', 'habidos', 'estés', 'vuestros', 'erais', 'hubiesen', 'tenía', 'me', 'habías', 'habrás', 'hay', 'tuvieras', 'tuvo', 'habida', 'quienes', 'fuesen', 'esté', 'tú', 'estarán', 'porque', 'tenían', 'estuvisteis', 'míos', 'los', 'eras', 'estaremos', 'antes', 'mi', 'fuésemos', 'sintiendo', 'sentida', 'tendrían', 'seré', 'vuestro', 'hayamos', 'estadas', 'era', 'tendrás', 'estará', 'estas', 'hube', 'fueses', 'te', 'estarías', 'al', 'habrá', 'tuvieses', 'habían', 'ya', 'habrías', 'el', 'ese', 'estarían', 'fuiste', 'estemos', 'tanto', 'soy', 'estuviésemos', 'tenidos', 'les', 'seáis', 'tendrías', 'fue', 'estuvieseis', 'algunas', 'hubiese', 'tenidas', 'fueseis', 'vosotros', 'sentido', 'a', 'estaba', 'para', 'este', 'nos', 'una', 'seamos', 'hubiera'}\n"
     ]
    }
   ],
   "source": [
    "sw_espanol = set(nltk.corpus.stopwords.words('spanish'))\n",
    "print(sw_espanol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b', 'd', 'e', 'a', 'c'}\n"
     ]
    }
   ],
   "source": [
    "my_list = {'a', 'b', 'c'}\n",
    "my_list.update(['d', 'e'])\n",
    "print(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is not a SW = Hello\n",
      "Is not a SW = ,\n",
      "Is not a SW = Everybody\n",
      "Is not a SW = .\n",
      "Is not a SW = Welcome\n",
      "Is not a SW = NLP\n",
      "Is not a SW = class\n",
      "Is not a SW = ,\n",
      "Is not a SW = hope\n",
      "Is not a SW = enjoy\n",
      "Is not a SW = lot\n",
      "Is not a SW = .\n",
      "Is not a SW = Remember\n",
      "Is not a SW = activities\n",
      "Is not a SW = assigned\n",
      "Is not a SW = homeworks\n",
      "Is not a SW = .\n",
      "Is not a SW = nice\n",
      "Is not a SW = day\n",
      "Is not a SW = !\n",
      "Is not a SW = !\n"
     ]
    }
   ],
   "source": [
    "for w in word:\n",
    "  if w.lower() not in sw_english:\n",
    "      print('Is not a SW =', w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No es SW = Hola\n",
      "No es SW = .\n",
      "No es SW = Bienvenidos\n",
      "No es SW = clase\n",
      "No es SW = LNP\n",
      "No es SW = ,\n",
      "No es SW = espero\n",
      "No es SW = disfruten\n",
      "No es SW = .\n",
      "No es SW = Recuerden\n",
      "No es SW = realizar\n",
      "No es SW = todas\n",
      "No es SW = actividades\n",
      "No es SW = tareas\n",
      "No es SW = asignadas\n",
      "No es SW = .\n",
      "No es SW = Espero\n",
      "No es SW = buen\n",
      "No es SW = día\n",
      "No es SW = !\n",
      "No es SW = !\n"
     ]
    }
   ],
   "source": [
    "for palabra in tokens_palabras:\n",
    "  if palabra.lower() not in sw_espanol:\n",
    "    print('No es SW =', palabra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvNaturalLenguageProcessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
