{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\efren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\efren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\efren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es-core-news-sm==3.7.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from es-core-news-sm==3.7.0) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (72.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2024.7.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\efren\\documents\\github\\venvnaturallenguageprocessing\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "# https://www.nltk.org/install.html\n",
    "# https://spacy.io/usage/models\n",
    "\n",
    "import nltk\n",
    "import nltk.data\n",
    "import spacy\n",
    "from nltk import SnowballStemmer\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.es.Spanish at 0x2f951589340>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.load('es_core_news_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text\n",
    "text = \"Hello, Everybody. Welcome to the NLP class, hope you enjoy it a lot. Remember to do all your activities and assigned homeworks. Have a nice day!!\"\n",
    "\n",
    "# texto\n",
    "texto = \"Hola a todos. Bienvenidos a la clase de LNP, espero lo disfruten mucho. Recuerden realizar todas sus actividades y tareas asignadas. Espero tengan un buen día!!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Everybody.\n",
      "Welcome to the NLP class, hope you enjoy it a lot.\n",
      "Remember to do all your activities and assigned homeworks.\n",
      "Have a nice day!\n",
      "!\n",
      "Hola a todos.\n",
      "Bienvenidos a la clase de LNP, espero lo disfruten mucho.\n",
      "Recuerden realizar todas sus actividades y tareas asignadas.\n",
      "Espero tengan un buen día!\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "# tokenize using sentences\n",
    "sentence = nltk.sent_tokenize(text)\n",
    "for sen in sentence:\n",
    "  print(sen)\n",
    "\n",
    "parrafo = nltk.sent_tokenize(texto)\n",
    "for parr in parrafo:\n",
    "  print(parr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      ",\n",
      "Everybody\n",
      ".\n",
      "Welcome\n",
      "to\n",
      "the\n",
      "NLP\n",
      "class\n",
      ",\n",
      "hope\n",
      "you\n",
      "enjoy\n",
      "it\n",
      "a\n",
      "lot\n",
      ".\n",
      "Remember\n",
      "to\n",
      "do\n",
      "all\n",
      "your\n",
      "activities\n",
      "and\n",
      "assigned\n",
      "homeworks\n",
      ".\n",
      "Have\n",
      "a\n",
      "nice\n",
      "day\n",
      "!\n",
      "!\n",
      "Hola\n",
      "a\n",
      "todos\n",
      ".\n",
      "Bienvenidos\n",
      "a\n",
      "la\n",
      "clase\n",
      "de\n",
      "LNP\n",
      ",\n",
      "espero\n",
      "lo\n",
      "disfruten\n",
      "mucho\n",
      ".\n",
      "Recuerden\n",
      "realizar\n",
      "todas\n",
      "sus\n",
      "actividades\n",
      "y\n",
      "tareas\n",
      "asignadas\n",
      ".\n",
      "Espero\n",
      "tengan\n",
      "un\n",
      "buen\n",
      "día\n",
      "!\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "# tokenize using words\n",
    "word = nltk.word_tokenize(text)\n",
    "for w in word:\n",
    "  print(w)\n",
    "\n",
    "palabra = nltk.word_tokenize(texto)\n",
    "for p in palabra:\n",
    "  print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Everybody\n",
      "Welcome\n",
      "to\n",
      "the\n",
      "NLP\n",
      "class\n",
      "hope\n",
      "you\n",
      "enjoy\n",
      "it\n",
      "a\n",
      "lot\n",
      "Remember\n",
      "to\n",
      "do\n",
      "all\n",
      "your\n",
      "activities\n",
      "and\n",
      "assigned\n",
      "homeworks\n",
      "Have\n",
      "a\n",
      "nice\n",
      "day\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\efren\\AppData\\Local\\Temp\\ipykernel_28328\\1563630428.py:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  word_regexp = nltk.regexp_tokenize(text, \"[\\w]+\") # \"[\\w]+\" read only words\n"
     ]
    }
   ],
   "source": [
    "# tokenize using regulat expressions\n",
    "word_regexp = nltk.regexp_tokenize(text, \"[\\w]+\") # \"[\\w]+\" read only words\n",
    "for w in word_regexp:\n",
    "  print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola\n",
      "a\n",
      "todos\n",
      "Bienvenidos\n",
      "a\n",
      "la\n",
      "clase\n",
      "de\n",
      "LNP\n",
      "espero\n",
      "lo\n",
      "disfruten\n",
      "mucho\n",
      "Recuerden\n",
      "realizar\n",
      "todas\n",
      "sus\n",
      "actividades\n",
      "y\n",
      "tareas\n",
      "asignadas\n",
      "Espero\n",
      "tengan\n",
      "un\n",
      "buen\n",
      "día\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\efren\\AppData\\Local\\Temp\\ipykernel_28328\\1159826405.py:2: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  palabra_regexp = nltk.regexp_tokenize(texto, \"[\\w]+\") # \"[\\w]+\" read only words\n"
     ]
    }
   ],
   "source": [
    "## regular expressions for spanish, a good practice to keep in mind\n",
    "palabra_regexp = nltk.regexp_tokenize(texto, \"[\\w]+\") # \"[\\w]+\" read only words\n",
    "for p in palabra_regexp:\n",
    "  print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola\n",
      "a\n",
      "todos\n",
      ".\n",
      "Bienvenidos\n",
      "a\n",
      "la\n",
      "clase\n",
      "de\n",
      "LNP\n",
      ",\n",
      "espero\n",
      "lo\n",
      "disfruten\n",
      "mucho\n",
      ".\n",
      "Recuerden\n",
      "realizar\n",
      "todas\n",
      "sus\n",
      "actividades\n",
      "y\n",
      "tareas\n",
      "asignadas\n",
      ".\n",
      "Espero\n",
      "tengan\n",
      "un\n",
      "buen\n",
      "día\n",
      "!\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "tokens_palabras = nltk.word_tokenize(texto)\n",
    "for w in tokens_palabras:\n",
    "  print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'against', 'how', 'doesn', 'nor', \"isn't\", 'by', \"wouldn't\", 'y', 'shan', 'off', 'that', 'ours', 'than', 'had', 'been', 'being', \"wasn't\", 'an', 'their', 'have', \"shouldn't\", 'if', 'hadn', 'hers', 'don', \"she's\", 'where', 're', 'having', 'am', 'again', 'about', 'too', 'she', 'between', 'shouldn', 'i', 'has', 'own', 'once', \"mightn't\", 'after', 'my', 'more', 'no', 'ourselves', 'only', 'down', 'm', 'mightn', 'was', 'me', 'few', 'both', 'this', 've', 'he', 'can', 'just', 'very', 'or', 'why', 'we', 'while', 'now', 'they', 'aren', 'myself', 'its', 'do', 'are', 'will', 'above', 'through', 's', 'itself', 'such', 'be', 'when', 'some', 'each', \"you'll\", \"needn't\", \"shan't\", 'at', \"didn't\", 'you', 'isn', 'what', 'there', 'is', 'from', 'theirs', 'up', 'over', 'a', 'herself', 'for', 'couldn', 'these', 'other', 'those', 'it', \"should've\", \"you'd\", \"that'll\", \"hadn't\", 'did', 'until', 'before', 'her', 'who', 'during', 'in', \"haven't\", 'hasn', \"weren't\", 'didn', 'on', 'but', \"hasn't\", 'of', 'whom', 'o', 'as', 'wasn', 'same', 'him', \"you've\", 'here', 'and', 'does', 'our', 'so', 'haven', \"aren't\", 'ma', 'wouldn', 'your', \"won't\", 'his', 'them', \"it's\", 'ain', 'any', 'needn', 'won', 'the', 'mustn', 'were', 'below', 'should', \"couldn't\", 'to', 'further', 'yours', 'which', 'because', 'with', 't', 'not', 'doing', 'into', 'all', 'weren', 'then', \"mustn't\", 'yourself', 'out', 'll', \"doesn't\", 'most', \"don't\", 'himself', 'd', 'yourselves', \"you're\", 'themselves', 'under'}\n"
     ]
    }
   ],
   "source": [
    "sw_english = set(nltk.corpus.stopwords.words('english'))\n",
    "print(sw_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'antes', 'sobre', 'nos', 'otra', 'estaríais', 'estará', 'un', 'estuvieran', 'la', 'esta', 'estabais', 'fuésemos', 'tuvo', 'fueron', 'tuyo', 'fue', 'mías', 'tienes', 'estoy', 'se', 'estéis', 'será', 'nada', 'tú', 'me', 'estuvo', 'tuviese', 'esté', 'estén', 'habías', 'estuviste', 'os', 'estuvieses', 'tuyos', 'sintiendo', 'sentid', 'tendrías', 'estaréis', 'estadas', 'hayáis', 'hubiéramos', 'tuvieras', 'tengamos', 'hubierais', 'hubiese', 'tanto', 'ella', 'hubieses', 'sea', 'estemos', 'habían', 'nuestra', 'seremos', 'estuvieras', 'otros', 'estaríamos', 'hay', 'o', 'hubieron', 'fui', 'teníais', 'fueses', 'como', 'fuera', 'por', 'había', 'tenemos', 'muy', 'era', 'son', 'ha', 'durante', 'nosotros', 'lo', 'con', 'soy', 'estados', 'habida', 'tengáis', 'ante', 'sean', 'hubieras', 'tendrás', 'pero', 'hubiésemos', 'estuviese', 'hasta', 'algunos', 'una', 'estuviera', 'han', 'habríamos', 'tengas', 'estuviesen', 'cuando', 'fuese', 'vosotros', 'estaremos', 'has', 'nuestras', 'sí', 'algunas', 'tuvierais', 'tendré', 'tu', 'otras', 'le', 'estarán', 'tengan', 'poco', 'vuestra', 'tuve', 'he', 'ya', 'de', 'el', 'eras', 'tuvieron', 'suya', 'mío', 'seamos', 'porque', 'en', 'habíais', 'hubisteis', 'estas', 'a', 'seáis', 'las', 'fueras', 'fuéramos', 'tened', 'fuerais', 'habrás', 'estamos', 'qué', 'contra', 'nuestro', 'estaba', 'otro', 'estad', 'estarías', 'tenida', 'seas', 'estabas', 'cual', 'seríais', 'seréis', 'teniendo', 'al', 'habremos', 'míos', 'para', 'mí', 'estuvieseis', 'suyo', 'teníamos', 'entre', 'estar', 'fuesen', 'uno', 'hayan', 'del', 'hubieseis', 'seríamos', 'y', 'tuviste', 'habiendo', 'éramos', 'tienen', 'tenéis', 'tenga', 'hube', 'fuisteis', 'sin', 'estos', 'habrías', 'tenidos', 'fuimos', 'sentidas', 'habré', 'estaré', 'hemos', 'mi', 'los', 'estaban', 'somos', 'erais', 'suyos', 'tendremos', 'estuviéramos', 'unos', 'que', 'tendríais', 'habrían', 'estás', 'tuvimos', 'tuvieseis', 'su', 'hayas', 'fuiste', 'él', 'tuvisteis', 'esa', 'estuviésemos', 'estuve', 'quien', 'habrán', 'les', 'más', 'siente', 'mis', 'desde', 'tuviera', 'muchos', 'quienes', 'estábamos', 'tendríamos', 'vuestras', 'tendrían', 'estarás', 'ellas', 'tenían', 'sus', 'habría', 'sentida', 'serán', 'hayamos', 'vuestro', 'esas', 'hubieran', 'tenido', 'eres', 'está', 'esto', 'habrá', 'estaría', 'nuestros', 'tenidas', 'sentidos', 'tenías', 'habido', 'haya', 'sois', 'yo', 'e', 'te', 'estada', 'ti', 'estuvieron', 'es', 'todo', 'mucho', 'tendría', 'ese', 'no', 'habréis', 'también', 'eran', 'tengo', 'habidos', 'tuvieses', 'fueran', 'donde', 'tendrá', 'suyas', 'serían', 'todos', 'estado', 'algo', 'sería', 'nosotras', 'hubiste', 'tuviésemos', 'estuvimos', 'seré', 'estarían', 'tuvieran', 'hubimos', 'tuviesen', 'habidas', 'tuya', 'tenía', 'hubiera', 'ni', 'habríais', 'tendréis', 'ellos', 'estés', 'están', 'sentido', 'vuestros', 'hubo', 'estuvierais', 'esos', 'mía', 'tendrán', 'habéis', 'tiene', 'tus', 'estuvisteis', 'este', 'serías', 'fueseis', 'habíamos', 'tuviéramos', 'estando', 'tuyas', 'hubiesen', 'serás', 'eso', 'estáis', 'vosotras'}\n"
     ]
    }
   ],
   "source": [
    "sw_espanol = set(nltk.corpus.stopwords.words('spanish'))\n",
    "print(sw_espanol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'d', 'c', 'b', 'e', 'a'}\n"
     ]
    }
   ],
   "source": [
    "my_list = {'a', 'b', 'c'}\n",
    "my_list.update(['d', 'e'])\n",
    "print(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is not a SW = Hello\n",
      "Is not a SW = ,\n",
      "Is not a SW = Everybody\n",
      "Is not a SW = .\n",
      "Is not a SW = Welcome\n",
      "Is not a SW = NLP\n",
      "Is not a SW = class\n",
      "Is not a SW = ,\n",
      "Is not a SW = hope\n",
      "Is not a SW = enjoy\n",
      "Is not a SW = lot\n",
      "Is not a SW = .\n",
      "Is not a SW = Remember\n",
      "Is not a SW = activities\n",
      "Is not a SW = assigned\n",
      "Is not a SW = homeworks\n",
      "Is not a SW = .\n",
      "Is not a SW = nice\n",
      "Is not a SW = day\n",
      "Is not a SW = !\n",
      "Is not a SW = !\n"
     ]
    }
   ],
   "source": [
    "for w in word:\n",
    "  if w.lower() not in sw_english:\n",
    "      print('Is not a SW =', w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No es SW = Hola\n",
      "No es SW = .\n",
      "No es SW = Bienvenidos\n",
      "No es SW = clase\n",
      "No es SW = LNP\n",
      "No es SW = ,\n",
      "No es SW = espero\n",
      "No es SW = disfruten\n",
      "No es SW = .\n",
      "No es SW = Recuerden\n",
      "No es SW = realizar\n",
      "No es SW = todas\n",
      "No es SW = actividades\n",
      "No es SW = tareas\n",
      "No es SW = asignadas\n",
      "No es SW = .\n",
      "No es SW = Espero\n",
      "No es SW = buen\n",
      "No es SW = día\n",
      "No es SW = !\n",
      "No es SW = !\n"
     ]
    }
   ],
   "source": [
    "for palabra in tokens_palabras:\n",
    "  if palabra.lower() not in sw_espanol:\n",
    "    print('No es SW =', palabra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvNaturalLenguageProcessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
