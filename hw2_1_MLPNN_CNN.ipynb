{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np # to use numpy arrays instead of lists\n",
    "import pandas as pd # DataFrame (table)\n",
    "import matplotlib.pyplot as plt # to plot\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Flatten, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Conv1D, AveragePooling1D, MaxPooling1D\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID  Sentiment                                           Sentence\n",
      "0   0          0  According to Gran , the company has no plans t...\n",
      "1   1          1  For the last quarter of 2010 , Componenta 's n...\n",
      "2   2          1  In the third quarter of 2010 , net sales incre...\n",
      "3   3          1  Operating profit rose to EUR 13.1 mn from EUR ...\n",
      "4   4          1  Operating profit totalled EUR 21.1 mn , up fro...\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('archives/Sentiment_Stock_data.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "sentences = data['Sentence'].astype(str).values  # Ensure all elements are strings\n",
    "labels = data['Sentiment'].values\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# Pad the sequences\n",
    "max_length = max([len(x) for x in sequences])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Convert labels to numpy array\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvNaturalLenguageProcessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
